import streamlit as st
import cv2
import mediapipe as mp
import numpy as np
import tensorflow as tf
from PIL import Image, ImageFont, ImageDraw
from streamlit_webrtc import webrtc_streamer, VideoTransformerBase
import av

# Load model
@st.cache_resource
def load_model():
    return tf.keras.models.load_model("sign_language_model.h5")

model = load_model()

# Setup MediaPipe
mp_hands = mp.solutions.hands
hands = mp_hands.Hands(
    static_image_mode=False,
    max_num_hands=2,
    min_detection_confidence=0.3
)

sign_language_classes = [
    "", "", "", "", "", "", "", "", "", "", "", "", "", "", "Ø¹Ø°Ø±Ù‹Ø§",
    "", "Ø·Ø¹Ø§Ù…", "", "", "Ù…Ø±Ø­Ø¨Ù‹Ø§", "Ù…Ø³Ø§Ø¹Ø¯Ø©", "Ù…Ù†Ø²Ù„", "Ø£Ù†Ø§", "Ø£Ø­Ø¨Ùƒ", "", "", "",
    "", "", "Ù„Ø§", "", "", "Ù„Ùˆ Ø³Ù…Ø­Øª", "", "", "", "", "Ø´ÙƒØ±Ù‹Ø§", "", "", "",
    "", "", "Ù†Ø¹Ù…", ""
]

# Utility functions
def process_landmarks(hand_landmarks):
    return [coord for lm in hand_landmarks.landmark for coord in (lm.x, lm.y, lm.z)]

def pad_landmarks():
    return [0.0] * 63

def classify_gesture(frame):
    rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
    result = hands.process(rgb)
    print("ğŸ‘€ classify_gesture running...")

    if result.multi_hand_landmarks:
        print("ğŸ–ï¸ Hand landmarks detected.")
        landmarks = process_landmarks(result.multi_hand_landmarks[0])
        if len(result.multi_hand_landmarks) > 1:
            landmarks += process_landmarks(result.multi_hand_landmarks[1])
        else:
            landmarks += pad_landmarks()

        prediction = model.predict(np.array(landmarks).reshape(1, -1), verbose=0)
        class_id = np.argmax(prediction[0])
        gesture = sign_language_classes[class_id]
        confidence = prediction[0][class_id]

        print(f"ğŸ¯ Prediction: {gesture} | Confidence: {confidence:.2%}")
        return gesture, result.multi_hand_landmarks, confidence

    print("âŒ No hand landmarks detected.")
    return None, None, None

def draw_text_with_arabic(frame, text, position, font_path="arial.ttf", font_size=48, color=(0, 255, 0)):
    img_pil = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))
    draw = ImageDraw.Draw(img_pil)
    try:
        font = ImageFont.truetype(font_path, font_size)
    except OSError:
        font = ImageFont.load_default()
        print("âš ï¸ arial.ttf not found. Using default font.")

    bbox = draw.textbbox((0, 0), text, font=font)
    pos = (position[0] - (bbox[2] - bbox[0]) // 2, position[1] - (bbox[3] - bbox[1]) // 2)
    draw.text(pos, text, font=font, fill=color)
    return cv2.cvtColor(np.array(img_pil), cv2.COLOR_RGB2BGR)

def process_uploaded_image(image_bytes):
    return cv2.imdecode(np.frombuffer(image_bytes, np.uint8), cv2.IMREAD_COLOR)

# ğŸ”§ Updated Video Processor
class VideoProcessor(VideoTransformerBase):
    def recv(self, frame: av.VideoFrame) -> av.VideoFrame:
        img = frame.to_ndarray(format="bgr24")
        print("ğŸ“¸ Frame received for processing")

        gesture, landmarks, conf = classify_gesture(img)

        if landmarks:
            for lm in landmarks:
                mp.solutions.drawing_utils.draw_landmarks(img, lm, mp_hands.HAND_CONNECTIONS)
        if gesture:
            img = draw_text_with_arabic(img, f"Ø§Ù„Ø¥Ø´Ø§Ø±Ø©: {gesture}", (img.shape[1] // 2, 50))

        return av.VideoFrame.from_ndarray(img, format="bgr24")

# UI
def main():
    st.title("Ù†Ø¸Ø§Ù… Ø§Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ù„ØºØ© Ø§Ù„Ø¥Ø´Ø§Ø±Ø© Ù„Ù„ØµÙ… ÙˆØ§Ù„Ø¨ÙƒÙ…")
    source = st.radio("Ø§Ø®ØªØ± Ù…ØµØ¯Ø± Ø§Ù„Ø¥Ø¯Ø®Ø§Ù„:", ["ÙƒØ§Ù…ÙŠØ±Ø§ Ø§Ù„ÙˆÙŠØ¨", "ØªØ­Ù…ÙŠÙ„ ØµÙˆØ±Ø©"])

    if source == "ØªØ­Ù…ÙŠÙ„ ØµÙˆØ±Ø©":
        file = st.file_uploader("Ø§Ø®ØªØ± ØµÙˆØ±Ø©", type=['jpg', 'jpeg', 'png'])
        if file:
            img = process_uploaded_image(file.read())
            gesture, landmarks, conf = classify_gesture(img)
            if landmarks:
                for lm in landmarks:
                    mp.solutions.drawing_utils.draw_landmarks(img, lm, mp_hands.HAND_CONNECTIONS)
            if gesture:
                img = draw_text_with_arabic(img, f"Ø§Ù„Ø¥Ø´Ø§Ø±Ø©: {gesture}", (img.shape[1] // 2, 50))
                st.write(f"Ø§Ù„Ø¥Ø´Ø§Ø±Ø© Ø§Ù„Ù…ÙƒØªØ´ÙØ©: {gesture}")
                st.write(f"Ù†Ø³Ø¨Ø© Ø§Ù„Ø«Ù‚Ø©: {conf:.2%}")
            else:
                st.write("Ù„Ù… ÙŠØªÙ… Ø§Ù„ÙƒØ´Ù Ø¹Ù† Ø£ÙŠ Ø¥Ø´Ø§Ø±Ø©")
            st.image(cv2.cvtColor(img, cv2.COLOR_BGR2RGB), caption="Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©", use_column_width=True)
    else:
        st.write("Ø§Ø¶ØºØ· Ø¹Ù„Ù‰ Ø²Ø± Ø§Ù„Ø¥ÙŠÙ‚Ø§Ù Ù„Ø¥Ù†Ù‡Ø§Ø¡ Ø§Ù„Ø¹Ø±Ø¶")
        webrtc_streamer(key="camera", video_processor_factory=VideoProcessor)  # âœ… updated argument

if __name__ == "__main__":
    main()
